{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Tableau CSVs adjustments\n",
    "#Load the 3 exported CSVs\n",
    "lr_results = pd.read_csv(\"lr_results.csv\")\n",
    "rf_results = pd.read_csv(\"rf_results.csv\")\n",
    "xgb_results = pd.read_csv(\"xgb_results.csv\")\n",
    "\n",
    "#Merge on CustomerId + Actual\n",
    "combined_results = lr_results.merge(rf_results, on=[\"CustomerId\", \"Actual\"])\n",
    "combined_results = combined_results.merge(xgb_results, on=[\"CustomerId\", \"Actual\"])\n",
    "\n",
    "#Save final dataset for Tableau\n",
    "combined_results.to_csv(\"bank_churn_model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Combined results saved as bank_churn_model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c214b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tableau data\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"bank_churn_model_comparison.csv\")\n",
    "\n",
    "models = {\n",
    "    \"LR\": (\"LR_pred\", \"LR_prob\"),\n",
    "    \"RF\": (\"RF_pred\", \"RF_prob\"),\n",
    "    \"XGB\": (\"XGB_pred\", \"XGB_prob\")\n",
    "}\n",
    "\n",
    "metrics = []\n",
    "roc_rows = []\n",
    "\n",
    "for name, (pred_col, prob_col) in models.items():\n",
    "    y_true = df[\"Actual\"].values\n",
    "    y_pred = df[pred_col].values\n",
    "    y_prob = df[prob_col].values\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "        \"ROC_AUC\": auc,\n",
    "        \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
    "    })\n",
    "\n",
    "    fpr, tpr, thresh = roc_curve(y_true, y_prob)\n",
    "    for f, t, th in zip(fpr, tpr, thresh):\n",
    "        roc_rows.append({\"Model\": name, \"FPR\": f, \"TPR\": t, \"Threshold\": th})\n",
    "\n",
    "pd.DataFrame(metrics).to_csv(\"model_metrics.csv\", index=False)\n",
    "pd.DataFrame(roc_rows).to_csv(\"roc_coords.csv\", index=False)\n",
    "print(\"Saved model_metrics.csv and roc_coords.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
